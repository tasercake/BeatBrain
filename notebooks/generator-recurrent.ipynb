{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# misc\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from fractions import Fraction\n",
    "\n",
    "# scientific\n",
    "import numpy as np\n",
    "import beatbrain\n",
    "from beatbrain import utils\n",
    "\n",
    "# visualization\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Sequential, Input, optimizers\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Lambda,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Layer,\n",
    "    Add,\n",
    "    Multiply,\n",
    "    Bidirectional,\n",
    "    BatchNormalization,\n",
    "    ReLU,\n",
    "    Activation,\n",
    "    Permute,\n",
    "    RepeatVector,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    Callback,\n",
    "    TensorBoard,\n",
    "    ReduceLROnPlateau,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TerminateOnNaN,\n",
    "    CSVLogger,\n",
    "    LambdaCallback,\n",
    ")\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "DATA_ROOT = Path(\"../data/fma/image\")\n",
    "IMAGE_DIMS = [512, 640, 1]\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = utils.load_dataset(\n",
    "    DATA_ROOT / \"train\", batch_size=BATCH_SIZE, parallel=False,\n",
    ")\n",
    "val_dataset = utils.load_dataset(\n",
    "    DATA_ROOT / \"val\", batch_size=BATCH_SIZE, parallel=False,\n",
    ")\n",
    "test_dataset = utils.load_dataset(\n",
    "    DATA_ROOT / \"test\", batch_size=1, parallel=False, shuffle_buffer=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2.0 * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -0.5 * ((sample - mean) ** 2.0 * tf.exp(-logvar) + logvar + log2pi), axis=raxis\n",
    "    )\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample(latent_dim, decoder, eps=None):\n",
    "    if eps is None:\n",
    "        eps = tf.random.normal(shape=(100, latent_dim))\n",
    "    return decode(decoder, eps, apply_sigmoid=True)\n",
    "\n",
    "\n",
    "def encode(encoder, x):\n",
    "    inference = encoder(x)\n",
    "    mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "\n",
    "def reparameterize(mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "\n",
    "def decode(decoder, z, apply_sigmoid=False):\n",
    "    logits = decoder(z)\n",
    "    if apply_sigmoid:\n",
    "        probs = tf.sigmoid(logits)\n",
    "        return probs\n",
    "    return logits\n",
    "\n",
    "\n",
    "def tanh_to_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Convert a tensor from `tanh` activation to `sigmoid` activation\n",
    "    \"\"\"\n",
    "    return ((x / 2) + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layers\n",
    "class ReconstructionLoss(Layer):\n",
    "    # TODO: subclass `Loss` instead\n",
    "    def __init__(self, mean=True):\n",
    "        self._mean = mean\n",
    "        super().__init__()\n",
    "        \n",
    "    def _mse(self, x):\n",
    "        return tf.reduce_sum(tf.losses.mse(x[0], x[1]), axis=[1, 2])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._model = Sequential([\n",
    "            Lambda(self._mse)\n",
    "        ])\n",
    "        if self._mean:\n",
    "            self._model.add(Lambda(tf.reduce_mean))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model.compute_output_shape(input_shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"mean\": self._mean,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class KLLoss(Layer):\n",
    "    # TODO: subclass `Loss` instead\n",
    "    def __init__(self, mean=True):\n",
    "        self._mean = mean\n",
    "        super().__init__()\n",
    "\n",
    "    def _log_normal_pdf(self, sample, mean, logvar, raxis=1):\n",
    "        log2pi = tf.math.log(2.0 * np.pi)\n",
    "        return tf.reduce_sum(\n",
    "            -0.5 * ((sample - mean) ** 2.0 * tf.exp(-logvar) + logvar + log2pi), axis=raxis\n",
    "        )\n",
    "\n",
    "    def _kld(self, x):\n",
    "        return self._log_normal_pdf(x[0], x[1], x[2]) - self._log_normal_pdf(x[0], 0., 0.)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._model = Sequential([\n",
    "            Lambda(self._kld)\n",
    "        ])\n",
    "        if self._mean:\n",
    "            self._model.add(Lambda(tf.reduce_mean))\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        z, z_mean, z_log_var = x\n",
    "        return self._model([z, z_mean, z_log_var])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model.compute_output_shape(input_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            \"mean\": self._mean,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "class Reparameterize(Layer):\n",
    "    def build(self, input_shape):\n",
    "        print(\"Reparam Layer input shape:\", input_shape)\n",
    "        inputs = Input(shape=[x[1:] for x in input_shape])\n",
    "        print(\"Reparam input shape:\", inputs.shape)\n",
    "        # TODO: Get rid of lambda expressions inside Lambda layers\n",
    "        epsilon = Lambda(lambda x: tf.keras.backend.random_normal(shape=tf.shape(x[0])))(inputs)\n",
    "        print(\"Epsilon shape:\", epsilon.shape)\n",
    "        mean = Lambda(lambda x: x[0])(inputs)\n",
    "        print(\"Mean shape:\", mean.shape)\n",
    "        var = Lambda(lambda x: tf.exp(x[1] * 0.5))(inputs)\n",
    "        print(\"Var shape:\", var.shape)\n",
    "        reparam = Multiply()([epsilon, var])\n",
    "        print(\"Mul shape:\", reparam.shape)\n",
    "        reparam = Add()([reparam, mean])\n",
    "        print(\"Add shape:\", reparam.shape)\n",
    "        self._model = Model(inputs=inputs, outputs=reparam)\n",
    "        print(\"Reparam output shape:\", self._model.output_shape)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        z_mean, z_log_var = x\n",
    "        output = self._model([z_mean, z_log_var])\n",
    "        print(\"Call time shape:\", output.shape)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model.compute_output_shape(input_shape)\n",
    "\n",
    "\n",
    "def build_encoder(input_shape, latent_dim, batch_size, repeat, use_inception):\n",
    "    def reparam(args):\n",
    "        z_mean, z_log_var = args\n",
    "        dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n",
    "        return eps * tf.exp(z_log_var * 0.5) + z_mean\n",
    "\n",
    "    encoder_input = Input(\n",
    "        shape=input_shape,\n",
    "        name=\"encoder_input\"\n",
    "    )\n",
    "    e = Reshape(input_shape[:-1])(encoder_input)  # Equivalent to squeezing channels axis\n",
    "    e = Permute((2, 1))(e)  # Convert to time-major format [batch, time, freq]\n",
    "    e = Bidirectional(LSTM(latent_dim))(e)\n",
    "    z_mean = Dense(latent_dim, name=\"z_mean\")(e)\n",
    "    z_log_var = Dense(latent_dim, name=\"z_log_var\")(e)\n",
    "    z = Lambda(reparam, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
    "    encoder = Model(inputs=encoder_input, outputs=[z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder_input, encoder\n",
    "\n",
    "\n",
    "def build_decoder(latent_dim, output_shape, batch_size, repeat, use_inception):\n",
    "    decoder_input = Input(\n",
    "        shape=latent_dim,\n",
    "        name=\"decoder_input\"\n",
    "    )\n",
    "    d = Dense(latent_dim)(decoder_input)\n",
    "    d = RepeatVector(output_shape[1])(d)\n",
    "    d = Bidirectional(LSTM(output_shape[0], return_sequences=True))(d)\n",
    "    d = Lambda(tanh_to_sigmoid)(d)\n",
    "    d = Permute((2, 1))(d)\n",
    "    d = Reshape(output_shape)(d)\n",
    "#     d = Activation(tf.nn.sigmoid)(d)\n",
    "    decoder = Model(inputs=decoder_input, outputs=d, name=\"decoder\")\n",
    "    return decoder_input, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_cvae(\n",
    "    latent_dim,\n",
    "    input_shape,\n",
    "    repeat=1,\n",
    "    use_inception=True,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "):\n",
    "    encoder_input, encoder = build_encoder(input_shape, latent_dim, batch_size, repeat, use_inception)\n",
    "    decoder_input, decoder = build_decoder(latent_dim, input_shape, batch_size, repeat, use_inception)\n",
    "    z_mean, z_log_var, z = encoder(encoder_input)\n",
    "    decoder_output = decoder(z)\n",
    "    model = Model(encoder_input, decoder_output, name=\"vae\")\n",
    "\n",
    "    print(f\"Encoder input: {encoder_input.shape}\")\n",
    "    print(f\"Decoder output: {decoder_output.shape}\")\n",
    "    encoder_input.shape.assert_is_compatible_with(decoder_output.shape)\n",
    "#     assert encoder_input.shape.as_list() == decoder_output.shape.as_list()\n",
    "\n",
    "    reconstruction_loss = ReconstructionLoss(mean=True)([encoder_input, decoder_output])\n",
    "#     reconstruction_loss = tf.losses.mse(encoder_input, decoder_output)\n",
    "#     reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=[1, 2])\n",
    "    kl_loss = KLLoss(mean=True)([z, z_mean, z_log_var])\n",
    "#     logpz = log_normal_pdf(z, 0.0, 0.0)\n",
    "#     logqz_x = log_normal_pdf(z, z_mean, z_log_var)\n",
    "#     kl_loss = logqz_x - logpz\n",
    "    vae_loss = reconstruction_loss + kl_loss\n",
    "    model.add_loss(vae_loss)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    # model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss=lambda yt, yp: vae_loss)\n",
    "\n",
    "    model.add_metric(reconstruction_loss, aggregation=\"mean\", name=\"reconstruction_loss\")\n",
    "    model.add_metric(kl_loss, aggregation=\"mean\", name=\"kl_loss\")\n",
    "    return model, encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class VisualizeCallback(Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_dir,\n",
    "        latent_dim,\n",
    "        validation_data,\n",
    "        n_examples=4,\n",
    "        random_vectors=None,\n",
    "        heatmap=True,\n",
    "        distribution=False,\n",
    "        frequency=\"epoch\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_examples = n_examples\n",
    "        self.cmap = \"magma\" if heatmap else \"Greys\"\n",
    "        self.frequency = frequency\n",
    "        self.verbose = verbose\n",
    "        self.total_batch = 0\n",
    "        self.distribution = distribution\n",
    "        self.random_vectors = random_vectors or tf.random.normal(\n",
    "            shape=[n_examples, latent_dim]\n",
    "        )\n",
    "        self.fig = plt.figure()\n",
    "        self.samples = list(validation_data.unbatch().take(self.n_examples))\n",
    "\n",
    "        self.recon_raw = self.log_dir / \"raw\" / \"reconstructed\"\n",
    "        self.recon_png = self.log_dir / \"png\" / \"reconstructed\"\n",
    "        self.gen_raw = self.log_dir / \"raw\" / \"generated\"\n",
    "        self.gen_png = self.log_dir / \"png\" / \"generated\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.recon_raw.mkdir(exist_ok=True, parents=True)\n",
    "        self.recon_png.mkdir(exist_ok=True, parents=True)\n",
    "        self.gen_raw.mkdir(exist_ok=True, parents=True)\n",
    "        self.gen_png.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def _visualize_reconstruction(self, batch=None, epoch=None):\n",
    "        assert (batch is not None) or (epoch is not None)\n",
    "        fig = plt.figure(self.fig.number)\n",
    "        fig.set_size_inches(10, 4)\n",
    "        for i, sample in enumerate(self.samples):\n",
    "            fig.add_subplot(121)\n",
    "            sample = sample[None, :]\n",
    "            beatbrain.display.show_spec(\n",
    "                utils.denormalize_spectrogram(sample[0, ..., 0].numpy()),\n",
    "                title=\"Original\",\n",
    "                cmap=self.cmap,\n",
    "            )\n",
    "            fig.add_subplot(122)\n",
    "            reconstructed = self.model(sample)\n",
    "            beatbrain.display.show_spec(\n",
    "                utils.denormalize_spectrogram(reconstructed[0, ..., 0].numpy()),\n",
    "                title=\"Reconstructed\",\n",
    "                cmap=self.cmap,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            title = f\"recon_{i + 1}@{'epoch' if epoch else 'batch'}_{epoch or batch:06}\"\n",
    "            fig.suptitle(title)\n",
    "            fig.savefig(self.recon_png / f\"{title}.jpg\")\n",
    "            utils.save_image(\n",
    "                reconstructed[0, ..., 0].numpy(), self.recon_raw / f\"{title}.exr\",\n",
    "            )\n",
    "            fig.clear()\n",
    "\n",
    "        if self.distribution:\n",
    "            fig = plt.figure(self.fig.number)\n",
    "            fig.set_size_inches(5, 4)\n",
    "            sns.distplot(reconstructed[0, ..., 0].numpy().flatten(), ax=fig.add_subplot(111))\n",
    "            plt.show()\n",
    "            fig.clear()\n",
    "\n",
    "    def _visualize_generation(self, batch=None, epoch=None):\n",
    "        assert (batch is not None) or (epoch is not None)\n",
    "        decoder = self.model.get_layer(\"decoder\")\n",
    "        generated = decoder(self.random_vectors)\n",
    "        fig = plt.figure(self.fig.number)\n",
    "        fig.set_size_inches(5, 4)\n",
    "        for i, gen in enumerate(generated):\n",
    "            gen = gen[None, :]\n",
    "            title = f\"gen_{i + 1}@{'epoch' if epoch else 'batch'}_{epoch or batch:06}\"\n",
    "            beatbrain.display.show_spec(\n",
    "                utils.denormalize_spectrogram(gen[0, ..., 0].numpy()),\n",
    "                title=title,\n",
    "                cmap=self.cmap,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(self.gen_png / f\"{title}.jpg\")\n",
    "            utils.save_image(gen[0, ..., 0], self.gen_raw / f\"{title}.exr\")\n",
    "            fig.clear()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if self.frequency == \"epoch\":\n",
    "            self._visualize_reconstruction(epoch=epoch)\n",
    "            self._visualize_generation(epoch=epoch)\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if isinstance(self.frequency, int) and (self.total_batch % self.frequency == 0):\n",
    "            self._visualize_reconstruction(batch=self.total_batch)\n",
    "            self._visualize_generation(batch=self.total_batch)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.total_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LATENT_DIM = 64\n",
    "EPOCHS = 200\n",
    "MODULE_REPEAT = 4\n",
    "USE_INCEPTION = False\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Outputs\n",
    "MODEL_NAME = \"rvae_dense_1sample\"\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "LOG_DIR = Path(\"../logs\") / MODEL_NAME\n",
    "LOG_DIR.mkdir(exist_ok=True, parents=True)\n",
    "LOG_FREQUENCY = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\n",
    "    log_dir=LOG_DIR,\n",
    "    update_freq=LOG_FREQUENCY,\n",
    "    histogram_freq=1,\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(patience=2, factor=0.1, min_lr=1e-7, verbose=1,)\n",
    "early_stop = EarlyStopping(patience=5, verbose=1,)\n",
    "model_saver = ModelCheckpoint(\n",
    "    str(MODEL_DIR / MODEL_NAME), save_best_only=True, verbose=1,\n",
    ")\n",
    "visualizer = VisualizeCallback(\n",
    "    LOG_DIR,\n",
    "    LATENT_DIM,\n",
    "    val_dataset,\n",
    "    frequency=LOG_FREQUENCY,\n",
    "    distribution=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on FMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, encoder, decoder = build_cvae(\n",
    "    LATENT_DIM,\n",
    "    IMAGE_DIMS,\n",
    "    repeat=MODULE_REPEAT,\n",
    "    use_inception=USE_INCEPTION,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "model.summary()\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "plot_model(model, to_file=str(LOG_DIR / \"model.png\"), expand_nested=True, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "model.fit_generator(\n",
    "    train_dataset.take(1000),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard, model_saver, reduce_lr, visualizer,],\n",
    "    validation_data=val_dataset.take(1000),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OR - Overfit model on 1 batch (from the test dataset, to ensure the same sample each time)\n",
    "single_sample = tf.data.Dataset.from_tensor_slices(list(test_dataset.take(1)))\n",
    "model.fit_generator(\n",
    "    single_sample.repeat(LOG_FREQUENCY),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard, VisualizeCallback(LOG_DIR, LATENT_DIM, single_sample, frequency=LOG_FREQUENCY)],\n",
    "    validation_data=single_sample,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
