{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# scientific\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import beatbrain\n",
    "from beatbrain import utils\n",
    "\n",
    "# visualization\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "DATA_ROOT = Path(\"../data/fma/image\")\n",
    "TEST_SPLIT = 0.2\n",
    "SUBSET = 5\n",
    "PARALLEL=False\n",
    "\n",
    "# Hyperparameters\n",
    "IMAGE_DIMS = [512, 640, 1]\n",
    "BATCH_SIZE = 1\n",
    "LATENT_DIM = 256\n",
    "EPOCHS = 2\n",
    "NUM_CONV = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Output\n",
    "EXAMPLES_TO_GENERATE = 16\n",
    "INTERPOLATION_POINTS = 9\n",
    "OUTPUT_DIR = Path(\"../data/output/images\")\n",
    "MODEL_PATH = Path(\"model.h5\")\n",
    "TENSORBOARD_DIR = Path('tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input type(s) in \u001b[33m'/home/cds_data/fma/image'\u001b[39m...\n",
      "Determined input type to be \u001b[36m'IMAGE'\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = utils.load_dataset(\n",
    "    DATA_ROOT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_split=TEST_SPLIT,\n",
    "    parallel=PARALLEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "CPU times: user 1min 47s, sys: 2.18 s, total: 1min 49s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in train_dataset.take(SUBSET):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cvae(latent_dim, input_shape, num_conv=2, batch_size=1, learning_rate=1e-4):\n",
    "    def reparam(args):\n",
    "        z_mean, z_log_var = args\n",
    "        dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=(batch_size, dim))\n",
    "        return eps * tf.exp(z_log_var * .5) + z_mean\n",
    "    \n",
    "    def reshape(args):\n",
    "        x, to_shape = args\n",
    "        return tf.keras.backend.reshape(x, to_shape)\n",
    "    \n",
    "    encoder_input = tf.keras.Input(shape=input_shape, batch_size=batch_size)\n",
    "    target_shape = [batch_size*input_shape[0], input_shape[1], 1]\n",
    "    encoder_input = tf.keras.layers.Lambda(reshape, output_shape=target_shape)([encoder_input, target_shape]) # (batch_size*freq, time, 1)\n",
    "    e = tf.keras.layers.Conv1D(filters=3, kernel_size=3, strides=2, activation='relu')(encoder_input) # (batch_size*freq, new_steps, filter_size)\n",
    "\n",
    "    for i in range(num_conv):\n",
    "        e = tf.keras.layers.Conv1D(filters=6, kernel_size=3, strides=2, activation='relu')(e) # (batch_size*freq, new_steps, filters)\n",
    "    \n",
    "    target_shape = (batch_size, input_shape[0], e.shape[1], e.shape[2])\n",
    "    e = tf.keras.layers.Lambda(reshape, output_shape=target_shape)([e, target_shape]) # (batch_size, freq, new_steps, filters)\n",
    "    e = tf.keras.layers.Flatten()(e)\n",
    "    e = tf.keras.layers.Dense(latent_dim*2)(e) # (batch_size, latent_dim*2)\n",
    "    z_mean = tf.keras.layers.Dense(latent_dim)(e)\n",
    "    z_log_var = tf.keras.layers.Dense(latent_dim)(e)\n",
    "    z = tf.keras.layers.Lambda(reparam, output_shape=(latent_dim, ))([z_mean, z_log_var])\n",
    "    encoder = tf.keras.Model(encoder_input, [z_mean, z_log_var, z], name=\"Encoder\")\n",
    "\n",
    "    decoder_input_shape = (1, \n",
    "        input_shape[0] // (2 ** (num_conv + 1)),\n",
    "        input_shape[1] // (2 ** (num_conv + 1)),\n",
    "        64\n",
    "    )\n",
    "    # (decoder_input_shape): (32, 40, 64)\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim, ))\n",
    "    d = tf.keras.layers.Dense(\n",
    "        decoder_input_shape[1] * decoder_input_shape[2] * decoder_input_shape[3],\n",
    "        activation=tf.nn.relu,\n",
    "    )(decoder_input)\n",
    "    print(d.shape)\n",
    "    d = tf.keras.layers.Reshape(\n",
    "        target_shape=(\n",
    "            decoder_input_shape[1],\n",
    "            decoder_input_shape[2],\n",
    "            decoder_input_shape[3],\n",
    "            )\n",
    "    )(d)\n",
    "    # d.shape: (None, 31, 39, 64)\n",
    "    for i in range(num_conv):\n",
    "        d = tf.keras.layers.Conv2DTranspose(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            activation='relu',\n",
    "        )(d)\n",
    "    d = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        strides=(2, 2),\n",
    "        padding='SAME',\n",
    "        activation='relu',\n",
    "    )(d)\n",
    "    decoder_output = tf.keras.layers.Conv2DTranspose(\n",
    "        filters=1,\n",
    "        kernel_size=3,\n",
    "        strides=(1, 1),\n",
    "        padding='SAME',\n",
    "    )(d)\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoder_output, name=\"Decoder\")    \n",
    "    outputs = decoder(encoder(encoder_input)[2])\n",
    "    model = tf.keras.Model(encoder_input, outputs, name=\"VAE\")\n",
    "    \n",
    "    assert (encoder_input.shape == outputs.shape)\n",
    "    reconstruction_loss = tf.losses.mse(encoder_input, outputs)\n",
    "    reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=[1, 2])\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, z_mean, z_log_var)\n",
    "    kl_loss = logqz_x - logpz\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    model.add_loss(vae_loss)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    return model, encoder, decoder\n",
    "\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def sample(latent_dim, decoder, eps=None):\n",
    "    if eps is None:\n",
    "        eps = tf.random.normal(shape=(100, latent_dim))\n",
    "    return decode(decoder, eps, apply_sigmoid=True)\n",
    "\n",
    "\n",
    "def encode(encoder, x):\n",
    "    inference = encoder(x)\n",
    "    mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "\n",
    "def reparameterize(mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "\n",
    "def decode(decoder, z, apply_sigmoid=False):\n",
    "    logits = decoder(z)\n",
    "    if apply_sigmoid:\n",
    "        probs = tf.sigmoid(logits)\n",
    "        return probs\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(encoder, decoder, x):\n",
    "    mean, logvar = encoder(x)\n",
    "    z = reparameterize(mean, logvar)\n",
    "    x_logit = decoder(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3]) # reconstruction\n",
    "    logpz = log_normal_pdf(z, 0., 0.) # kl divergence loss 1 term\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar) # kl divergence loss term\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, encoder, decoder, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(encoder, decoder, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model inputs must come from `tf.keras.Input` (thus holding past layer metadata), they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"Encoder\" was not an Input tensor, it was generated by layer tf_op_layer_Reshape_39.\n",
      "Note that input tensors are instantiated via `tensor = tf.keras.Input(shape)`.\n",
      "The tensor that caused the issue was: Reshape_39:0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_44:0\", shape=(1, 512, 640, 1), dtype=float32) at layer \"input_44\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-cedcd6bbd4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# it will be easier to see the improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom_vector_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEXAMPLES_TO_GENERATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLATENT_DIM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLATENT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_DIMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CONV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-35532ee4b2a5>\u001b[0m in \u001b[0;36mbuild_cvae\u001b[0;34m(latent_dim, input_shape, num_conv, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mz_log_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Encoder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     decoder_input_shape = (1, \n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;31m# initializing _distribution_strategy here since it is possible to call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m    166\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0;32m--> 320\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1623\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_44:0\", shape=(1, 512, 640, 1), dtype=float32) at layer \"input_44\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random.normal(shape=[EXAMPLES_TO_GENERATE, LATENT_DIM])\n",
    "model, encoder, decoder = build_cvae(LATENT_DIM, IMAGE_DIMS, num_conv=NUM_CONV, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "    1/34656 [..............................] - ETA: 1083:01:00 - loss: 85054.5312WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.128298). Check your callbacks.\n",
      "23148/34656 [===================>..........] - ETA: 9:46 - loss: 4293.7407"
     ]
    }
   ],
   "source": [
    "# single_sample = utils.load_image('../data/fma/image/000/000002/0.exr')\n",
    "# single_sample = train_dataset.reshape(1, *single_sample.shape, 1)\n",
    "# model.fit(single_sample, epochs=EPOCHS, validation_data=(single_sample, None))\n",
    "\n",
    "# model.fit_generator(train_dataset, epochs=EPOCHS, validation_data=test_dataset)\n",
    "model.fit_generator(train_dataset, epochs=EPOCHS, callbacks=[\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=TENSORBOARD_DIR / str(int(time.time())),\n",
    "        update_freq=100,\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset\n",
    "\n",
    "beatbrain.display.show_spec(utils.denormalize_spectrogram(sample[0, ..., 0]), title=\"Orginal\")\n",
    "plt.show()\n",
    "sns.distplot(train_dataset.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(train_dataset)\n",
    "beatbrain.display.show_spec(utils.denormalize_spectrogram(model(sample)[0, ..., 0].numpy()), title=\"Predicted\")\n",
    "plt.show()\n",
    "sns.distplot(model(train_dataset).numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FFT = 4096\n",
    "HOP_LENGTH = 256\n",
    "SAMPLE_RATE = 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recon = utils.spectrogram_to_audio(sample[0, ..., 0], denormalize=True, n_fft=N_FFT, hop_length=HOP_LENGTH, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(sample_recon, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_recon = utils.spectrogram_to_audio(prediction[0, ..., 0], denormalize=True, n_fft=N_FFT, hop_length=HOP_LENGTH, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(prediction_recon, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(decoder, epoch, test_input):\n",
    "    num_plots = math.ceil(math.sqrt(len(test_input)))\n",
    "    predictions = sample(LATENT_DIM, decoder, eps=test_input)\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        plt.subplot(num_plots, num_plots, i + 1)\n",
    "        plt.imshow(pred[:, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        plt.axis('off')\n",
    "        output_dir = os.path.join(OUTPUT_DIR, 'progress', str(i))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        image = Image.fromarray(pred[:, :, 0].numpy(), mode='F')\n",
    "#         image.save(os.path.join(output_dir, f\"epoch_{epoch}.tiff\"))\n",
    "        image.save(os.path.join(output_dir, f\"spec.tiff\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import librosa\n",
    "\n",
    "SR = 32768\n",
    "N_FFT = 4096\n",
    "N_MELS = 512\n",
    "HOP_LENGTH = 256\n",
    "DURATION = 5\n",
    "A = \"../data/fma/audio/000002.mp3\"\n",
    "B = \"../data/fma/audio/000005.mp3\"\n",
    "\n",
    "interp_dir = Path(f\"interpolation/{int(time())}\")\n",
    "interp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "x, _ = librosa.load(A, sr=SR, duration=DURATION)\n",
    "y, _ = librosa.load(B, sr=SR, duration=DURATION)\n",
    "x = librosa.feature.melspectrogram(x, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "y = librosa.feature.melspectrogram(y, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "x, y = x[0, None], y[None, :, None]\n",
    "print(x.shape)\n",
    "x_mean, x_logvar = model.encode(x)\n",
    "y_mean, y_logvar = model.encode(y)\n",
    "\n",
    "# Reconstruction\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "x_recon = model.sample(model.reparameterize(x_mean, x_logvar))\n",
    "plt.imshow(x_recon[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Interpolation\n",
    "fractions = np.linspace(0, 1, num=INTERPOLATION_POINTS)[:, None]\n",
    "means = (x_mean * (1 - fractions)) + (y_mean * fractions)  # Interpolated latent vectors\n",
    "logvars = (x_logvar * (1 - fractions)) + (y_logvar * fractions)  # Interpolated latent vectors\n",
    "points = model.reparameterize(means, logvars)\n",
    "interpolated = model.sample(points)\n",
    "\n",
    "num_plots = math.ceil(math.sqrt(INTERPOLATION_POINTS))\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "fig.subplots_adjust(hspace=0, wspace=0.03)\n",
    "for i, pred in enumerate(interpolated):\n",
    "    Image.fromarray(pred[:, :, 0].numpy(), mode='F').save(interp_dir.joinpath(f'{i}.tiff'))\n",
    "    plt.subplot(num_plots, num_plots, i + 1)\n",
    "    if i == 0:\n",
    "        Image.fromarray(x[0, :, :, 0].numpy(), mode='F').save(interp_dir.joinpath(f'a.tiff'))\n",
    "        plt.imshow(x[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "    elif i == INTERPOLATION_POINTS - 1:\n",
    "        Image.fromarray(y[0, :, :, 0].numpy(), mode='F').save(interp_dir.joinpath(f'b.tiff'))\n",
    "        plt.imshow(y[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "    else:\n",
    "        plt.imshow(pred[:, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.savefig(interp_dir.joinpath('interpolation.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, num_conv=2, input_shape=(*IMAGE_DIMS, WINDOW_SIZE)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation='relu',\n",
    "                ),\n",
    "                *[tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'\n",
    "                ) for i in range(num_conv)],\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(2 * latent_dim)\n",
    "            ], name=\"Encoder\"\n",
    "        )\n",
    "\n",
    "        decoder_input_shape = (\n",
    "            input_shape[0] // (2 ** (num_conv + 1)),\n",
    "            input_shape[1] // (2 ** (num_conv + 1)),\n",
    "            64\n",
    "        )\n",
    "\n",
    "        self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim, )),\n",
    "                tf.keras.layers.Dense(\n",
    "                    decoder_input_shape[0] * decoder_input_shape[1] * decoder_input_shape[2],\n",
    "                    activation=tf.nn.relu\n",
    "                ),\n",
    "                tf.keras.layers.Reshape(target_shape=(decoder_input_shape[0], \n",
    "                                                      decoder_input_shape[1], decoder_input_shape[2])),\n",
    "                *[tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64,\n",
    "                    kernel_size=3,\n",
    "                    strides=(2, 2),\n",
    "                    padding='SAME',\n",
    "                    activation='relu'\n",
    "                ) for i in range(num_conv)],\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32,\n",
    "                    kernel_size=3,\n",
    "                    strides=(2, 2),\n",
    "                    padding='SAME',\n",
    "                    activation='relu'\n",
    "                ),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=(1, 1), padding='SAME'\n",
    "                )\n",
    "            ], name=\"Decoder\"\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        inference = self.inference_net(x)\n",
    "        mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(LATENT_DIM, num_conv=NUM_CONV,)\n",
    "model.inference_net.summary()\n",
    "model.generative_net.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
