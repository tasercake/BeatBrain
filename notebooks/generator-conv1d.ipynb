{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_formatted_code = \"# misc\\nimport math\\nimport time\\nfrom pathlib import Path\\n\\n# scientific\\nimport numpy as np\\nimport beatbrain\\nfrom beatbrain import utils\\n\\n# visualization\\nfrom IPython import display\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# misc\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# scientific\n",
    "import numpy as np\n",
    "import beatbrain\n",
    "from beatbrain import utils\n",
    "\n",
    "# visualization\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_formatted_code = \"# Tensorflow\\nimport tensorflow as tf\\n\\nfrom tensorflow.keras import Model, Sequential, Input\\nfrom tensorflow.keras import backend\\nfrom tensorflow.keras import optimizers\\n\\nfrom tensorflow.keras.layers import (\\n    Conv1D,\\n    Conv2D,\\n    Conv2DTranspose,\\n    MaxPool1D,\\n    MaxPool2D,\\n    Dense,\\n    Lambda,\\n    Reshape,\\n    Flatten,\\n    Layer,\\n    concatenate,\\n)\\nfrom tensorflow.keras.callbacks import (\\n    Callback,\\n    TensorBoard,\\n    ReduceLROnPlateau,\\n    EarlyStopping,\\n    ModelCheckpoint,\\n    TerminateOnNaN,\\n    CSVLogger,\\n    LambdaCallback,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Model, Sequential, Input\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv1D,\n",
    "    Conv2D,\n",
    "    Conv2DTranspose,\n",
    "    MaxPool1D,\n",
    "    MaxPool2D,\n",
    "    Dense,\n",
    "    Lambda,\n",
    "    Reshape,\n",
    "    Flatten,\n",
    "    Layer,\n",
    "    concatenate,\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    Callback,\n",
    "    TensorBoard,\n",
    "    ReduceLROnPlateau,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    TerminateOnNaN,\n",
    "    CSVLogger,\n",
    "    LambdaCallback,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_formatted_code = \"sns.set()\\nsns.set_style(\\\"white\\\")\\n%matplotlib inline\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_formatted_code = \"# Input\\nDATA_ROOT = Path(\\\"../data/fma/image\\\")\\nIMAGE_DIMS = [512, 640, 1]\\nBATCH_SIZE = 4\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input\n",
    "DATA_ROOT = Path(\"../data/fma/image\")\n",
    "IMAGE_DIMS = [512, 640, 1]\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input type(s) in \u001b[33m'/media/data/beatbrain_data/fma/image/train'\u001b[39m...\n",
      "Determined input type to be \u001b[36m'IMAGE'\u001b[39m\n",
      "Checking input type(s) in \u001b[33m'/media/data/beatbrain_data/fma/image/val'\u001b[39m...\n",
      "Determined input type to be \u001b[36m'IMAGE'\u001b[39m\n",
      "Checking input type(s) in \u001b[33m'/media/data/beatbrain_data/fma/image/test'\u001b[39m...\n",
      "Determined input type to be \u001b[36m'IMAGE'\u001b[39m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_formatted_code = \"train_dataset = utils.load_dataset(\\n    DATA_ROOT / \\\"train\\\", batch_size=BATCH_SIZE, parallel=False\\n)\\nval_dataset = utils.load_dataset(\\n    DATA_ROOT / \\\"val\\\", batch_size=BATCH_SIZE, parallel=False,\\n)\\ntest_dataset = utils.load_dataset(\\n    DATA_ROOT / \\\"test\\\", batch_size=1, parallel=False, shuffle_buffer=0,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = utils.load_dataset(\n",
    "    DATA_ROOT / \"train\", batch_size=BATCH_SIZE, parallel=False\n",
    ")\n",
    "val_dataset = utils.load_dataset(\n",
    "    DATA_ROOT / \"val\", batch_size=BATCH_SIZE, parallel=False,\n",
    ")\n",
    "test_dataset = utils.load_dataset(\n",
    "    DATA_ROOT / \"test\", batch_size=1, parallel=False, shuffle_buffer=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_formatted_code = \"def inception_1d(x, filters):\\n    conv1 = Conv1D(filters=filters, kernel_size=1, padding=\\\"SAME\\\")(x)\\n    conv3 = Conv1D(filters=filters, kernel_size=1, padding=\\\"SAME\\\")(x)\\n    conv3 = Conv1D(filters=filters, kernel_size=3, padding=\\\"SAME\\\")(conv3)\\n    conv5 = Conv1D(filters=filters, kernel_size=1, padding=\\\"SAME\\\")(x)\\n    conv5 = Conv1D(filters=filters, kernel_size=5, padding=\\\"SAME\\\")(conv5)\\n    pool = MaxPool1D(pool_size=3, strides=1, padding=\\\"SAME\\\")(x)\\n    pool = Conv1D(filters=filters, kernel_size=1, padding=\\\"SAME\\\")(pool)\\n    concat = concatenate([conv1, conv3, conv5, pool], axis=-1)\\n    return concat\\n\\n\\ndef build_cvae(\\n    latent_dim,\\n    input_shape,\\n    start_filters=1024,\\n    num_conv=3,\\n    num_inception=3,\\n    num_deconv=3,\\n    batch_size=1,\\n    learning_rate=1e-4,\\n):\\n    def reparam(args):\\n        z_mean, z_log_var = args\\n        dim = tf.keras.backend.int_shape(z_mean)[1]\\n        eps = tf.keras.backend.random_normal(shape=(batch_size, dim))\\n        return eps * tf.exp(z_log_var * 0.5) + z_mean\\n\\n    encoder_input = Input(shape=input_shape, batch_size=batch_size)\\n    e = Lambda(reshape_1d)(encoder_input)  # (batch_size, time, freq)\\n    for i in range(num_conv):\\n        e = Conv1D(\\n            filters=start_filters // (2 ** i),\\n            kernel_size=3,\\n            strides=1,\\n            padding=\\\"SAME\\\",\\n            activation=\\\"relu\\\",\\n        )(e)\\n    for i in range(num_inception):\\n        e = inception_1d(e, start_filters // 2 ** (num_conv - 1))\\n        e = MaxPool1D()(e)\\n    #     decoder_input_shape = tf.keras.backend.int_shape(e)\\n    e = Flatten()(e)\\n    e = Dense(latent_dim * 2)(e)  # (batch_size, latent_dim*2)\\n    z_mean = Dense(latent_dim)(e)\\n    z_log_var = Dense(latent_dim)(e)\\n    z = Lambda(reparam, output_shape=(latent_dim,))(\\n        [z_mean, z_log_var]\\n    )  # (batch_size, latent_dim)\\n    encoder = Model(encoder_input, [z_mean, z_log_var, z], name=\\\"encoder\\\")\\n\\n    decoder_input_shape = [\\n        batch_size,\\n        input_shape[1] // 2 ** num_deconv,\\n        input_shape[0] // 2 ** num_deconv,\\n    ]  # shape: [?, time, freq]\\n    decoder_input = Input(shape=(latent_dim,))\\n    d = Dense(decoder_input_shape[1] * decoder_input_shape[2], activation=\\\"relu\\\",)(\\n        decoder_input\\n    )\\n    d = Reshape(target_shape=(decoder_input_shape[1], decoder_input_shape[2]))(d)\\n    for i in range(num_deconv):\\n        d = Conv1DTranspose(\\n            filters=input_shape[0] // 2 ** (num_deconv - i),\\n            kernel_size=3,\\n            strides=2,\\n            padding=\\\"SAME\\\",\\n            activation=\\\"relu\\\",\\n        )(d)\\n    decoder_output = Conv1DTranspose(\\n        filters=input_shape[0],\\n        kernel_size=3,\\n        strides=1,\\n        padding=\\\"SAME\\\",\\n        activation=\\\"relu\\\",\\n    )(d)\\n    decoder_output = Lambda(reshape_2d)(decoder_output)\\n    decoder = Model(decoder_input, decoder_output, name=\\\"decoder\\\")\\n    outputs = decoder(encoder(encoder_input)[2])  # shape: [?, time, freq]\\n    d = Lambda(reshape_2d)(d)\\n    model = Model(\\n        encoder_input, outputs, name=\\\"vae\\\"\\n    )  # shape: [?, freq, time, channels]\\n\\n    assert encoder_input.shape == outputs.shape\\n    reconstruction_loss = tf.losses.mse(encoder_input, outputs)\\n    reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=[1, 2])\\n    log2pi = tf.math.log(2.0 * np.pi)\\n    logpz = log_normal_pdf(z, 0.0, 0.0)\\n    logqz_x = log_normal_pdf(z, z_mean, z_log_var)\\n    kl_loss = logqz_x - logpz\\n    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\\n\\n    model.add_loss(vae_loss)\\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\\n    return model, encoder, decoder\\n\\n\\nclass Conv1DTranspose(Layer):\\n    def __init__(self, filters, kernel_size, strides=1, *args, **kwargs):\\n        self._filters = filters\\n        self._kernel_size = (1, kernel_size)\\n        self._strides = (1, strides)\\n        self._args, self._kwargs = args, kwargs\\n        super(Conv1DTranspose, self).__init__()\\n\\n    def build(self, input_shape):\\n        self._model = Sequential()\\n        self._model.add(\\n            Lambda(lambda x: tf.expand_dims(x, 1), batch_input_shape=input_shape)\\n        )\\n        self._model.add(\\n            Conv2DTranspose(\\n                self._filters,\\n                kernel_size=self._kernel_size,\\n                strides=self._strides,\\n                *self._args,\\n                **self._kwargs\\n            )\\n        )\\n        self._model.add(Lambda(lambda x: x[:, 0]))\\n        super(Conv1DTranspose, self).build(input_shape)\\n\\n    def call(self, x):\\n        return self._model(x)\\n\\n    def compute_output_shape(self, input_shape):\\n        return self._model.compute_output_shape(input_shape)\\n\\n\\ndef log_normal_pdf(sample, mean, logvar, raxis=1):\\n    log2pi = tf.math.log(2.0 * np.pi)\\n    return tf.reduce_sum(\\n        -0.5 * ((sample - mean) ** 2.0 * tf.exp(-logvar) + logvar + log2pi), axis=raxis\\n    )\\n\\n\\n@tf.function\\ndef sample(latent_dim, decoder, eps=None):\\n    if eps is None:\\n        eps = tf.random.normal(shape=(100, latent_dim))\\n    return decode(decoder, eps, apply_sigmoid=True)\\n\\n\\ndef encode(encoder, x):\\n    inference = encoder(x)\\n    mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\\n    return mean, logvar\\n\\n\\ndef reparameterize(mean, logvar):\\n    eps = tf.random.normal(shape=mean.shape)\\n    return eps * tf.exp(logvar * 0.5) + mean\\n\\n\\ndef decode(decoder, z, apply_sigmoid=False):\\n    logits = decoder(z)\\n    if apply_sigmoid:\\n        probs = tf.sigmoid(logits)\\n        return probs\\n    return logits\\n\\n\\ndef reshape_1d(x, axis=-1):\\n    x = tf.squeeze(x, axis=-1)\\n    x = tf.transpose(x, [0, 2, 1])\\n    return x\\n\\n\\ndef reshape_2d(x, axis=-1):\\n    x = tf.transpose(x, [0, 2, 1])\\n    x = tf.expand_dims(x, axis=-1)\\n    return x\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def inception_1d(x, filters):\n",
    "    conv1 = Conv1D(filters=filters, kernel_size=1, padding=\"SAME\")(x)\n",
    "    conv3 = Conv1D(filters=filters, kernel_size=1, padding=\"SAME\")(x)\n",
    "    conv3 = Conv1D(filters=filters, kernel_size=3, padding=\"SAME\")(conv3)\n",
    "    conv5 = Conv1D(filters=filters, kernel_size=1, padding=\"SAME\")(x)\n",
    "    conv5 = Conv1D(filters=filters, kernel_size=5, padding=\"SAME\")(conv5)\n",
    "    pool = MaxPool1D(pool_size=3, strides=1, padding=\"SAME\")(x)\n",
    "    pool = Conv1D(filters=filters, kernel_size=1, padding=\"SAME\")(pool)\n",
    "    concat = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return concat\n",
    "\n",
    "\n",
    "def build_cvae(\n",
    "    latent_dim,\n",
    "    input_shape,\n",
    "    start_filters=1024,\n",
    "    num_conv=3,\n",
    "    num_inception=3,\n",
    "    num_deconv=3,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-4,\n",
    "):\n",
    "    def reparam(args):\n",
    "        z_mean, z_log_var = args\n",
    "        dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=(batch_size, dim))\n",
    "        return eps * tf.exp(z_log_var * 0.5) + z_mean\n",
    "\n",
    "    encoder_input = Input(shape=input_shape, batch_size=batch_size)\n",
    "    e = Lambda(reshape_1d)(encoder_input)  # (batch_size, time, freq)\n",
    "    for i in range(num_conv):\n",
    "        e = Conv1D(\n",
    "            filters=start_filters // (2 ** i),\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "        )(e)\n",
    "    for i in range(num_inception):\n",
    "        e = inception_1d(e, start_filters // 2 ** (num_conv - 1))\n",
    "        e = MaxPool1D()(e)\n",
    "    #     decoder_input_shape = tf.keras.backend.int_shape(e)\n",
    "    e = Flatten()(e)\n",
    "    e = Dense(latent_dim * 2)(e)  # (batch_size, latent_dim*2)\n",
    "    z_mean = Dense(latent_dim)(e)\n",
    "    z_log_var = Dense(latent_dim)(e)\n",
    "    z = Lambda(reparam, output_shape=(latent_dim,))(\n",
    "        [z_mean, z_log_var]\n",
    "    )  # (batch_size, latent_dim)\n",
    "    encoder = Model(encoder_input, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    decoder_input_shape = [\n",
    "        batch_size,\n",
    "        input_shape[1] // 2 ** num_deconv,\n",
    "        input_shape[0] // 2 ** num_deconv,\n",
    "    ]  # shape: [?, time, freq]\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    d = Dense(decoder_input_shape[1] * decoder_input_shape[2], activation=\"relu\",)(\n",
    "        decoder_input\n",
    "    )\n",
    "    d = Reshape(target_shape=(decoder_input_shape[1], decoder_input_shape[2]))(d)\n",
    "    for i in range(num_deconv):\n",
    "        d = Conv1DTranspose(\n",
    "            filters=input_shape[0] // 2 ** (num_deconv - i),\n",
    "            kernel_size=3,\n",
    "            strides=2,\n",
    "            padding=\"SAME\",\n",
    "            activation=\"relu\",\n",
    "        )(d)\n",
    "    decoder_output = Conv1DTranspose(\n",
    "        filters=input_shape[0],\n",
    "        kernel_size=3,\n",
    "        strides=1,\n",
    "        padding=\"SAME\",\n",
    "        activation=\"relu\",\n",
    "    )(d)\n",
    "    decoder_output = Lambda(reshape_2d)(decoder_output)\n",
    "    decoder = Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "    outputs = decoder(encoder(encoder_input)[2])  # shape: [?, time, freq]\n",
    "    d = Lambda(reshape_2d)(d)\n",
    "    model = Model(\n",
    "        encoder_input, outputs, name=\"vae\"\n",
    "    )  # shape: [?, freq, time, channels]\n",
    "\n",
    "    assert encoder_input.shape == outputs.shape\n",
    "    reconstruction_loss = tf.losses.mse(encoder_input, outputs)\n",
    "    reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=[1, 2])\n",
    "    log2pi = tf.math.log(2.0 * np.pi)\n",
    "    logpz = log_normal_pdf(z, 0.0, 0.0)\n",
    "    logqz_x = log_normal_pdf(z, z_mean, z_log_var)\n",
    "    kl_loss = logqz_x - logpz\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    model.add_loss(vae_loss)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    return model, encoder, decoder\n",
    "\n",
    "\n",
    "class Conv1DTranspose(Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=1, *args, **kwargs):\n",
    "        self._filters = filters\n",
    "        self._kernel_size = (1, kernel_size)\n",
    "        self._strides = (1, strides)\n",
    "        self._args, self._kwargs = args, kwargs\n",
    "        super(Conv1DTranspose, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._model = Sequential()\n",
    "        self._model.add(\n",
    "            Lambda(lambda x: tf.expand_dims(x, 1), batch_input_shape=input_shape)\n",
    "        )\n",
    "        self._model.add(\n",
    "            Conv2DTranspose(\n",
    "                self._filters,\n",
    "                kernel_size=self._kernel_size,\n",
    "                strides=self._strides,\n",
    "                *self._args,\n",
    "                **self._kwargs\n",
    "            )\n",
    "        )\n",
    "        self._model.add(Lambda(lambda x: x[:, 0]))\n",
    "        super(Conv1DTranspose, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model.compute_output_shape(input_shape)\n",
    "\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2.0 * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -0.5 * ((sample - mean) ** 2.0 * tf.exp(-logvar) + logvar + log2pi), axis=raxis\n",
    "    )\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def sample(latent_dim, decoder, eps=None):\n",
    "    if eps is None:\n",
    "        eps = tf.random.normal(shape=(100, latent_dim))\n",
    "    return decode(decoder, eps, apply_sigmoid=True)\n",
    "\n",
    "\n",
    "def encode(encoder, x):\n",
    "    inference = encoder(x)\n",
    "    mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "\n",
    "def reparameterize(mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * 0.5) + mean\n",
    "\n",
    "\n",
    "def decode(decoder, z, apply_sigmoid=False):\n",
    "    logits = decoder(z)\n",
    "    if apply_sigmoid:\n",
    "        probs = tf.sigmoid(logits)\n",
    "        return probs\n",
    "    return logits\n",
    "\n",
    "\n",
    "def reshape_1d(x, axis=-1):\n",
    "    x = tf.squeeze(x, axis=-1)\n",
    "    x = tf.transpose(x, [0, 2, 1])\n",
    "    return x\n",
    "\n",
    "\n",
    "def reshape_2d(x, axis=-1):\n",
    "    x = tf.transpose(x, [0, 2, 1])\n",
    "    x = tf.expand_dims(x, axis=-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters, Model Output, and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_formatted_code = \"# Hyperparameters\\nLATENT_DIM = 256\\nEPOCHS = 50\\nNUM_CONV = 3\\nLEARNING_RATE = 1e-4\\n\\n# Outputs\\nMODEL_NAME = \\\"cvae_1d_inception\\\"\\nMODEL_DIR = Path(\\\"../models\\\")\\nMODEL_DIR.mkdir(exist_ok=True, parents=True)\\nLOG_DIR = Path(\\\"../logs\\\") / MODEL_NAME\\nLOG_FREQUENCY = 200\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LATENT_DIM = 256\n",
    "EPOCHS = 50\n",
    "NUM_CONV = 3\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Outputs\n",
    "MODEL_NAME = \"cvae_1d_inception\"\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True, parents=True)\n",
    "LOG_DIR = Path(\"../logs\") / MODEL_NAME\n",
    "LOG_FREQUENCY = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_formatted_code = \"class VisualizeCallback(Callback):\\n    def __init__(\\n        self,\\n        log_dir,\\n        latent_dim,\\n        validation_data,\\n        n_examples=4,\\n        random_vectors=None,\\n        heatmap=True,\\n        frequency=\\\"epoch\\\",\\n        verbose=False,\\n    ):\\n        self.log_dir = Path(log_dir)\\n        self.latent_dim = latent_dim\\n        self.n_examples = n_examples\\n        self.cmap = None if heatmap else \\\"Greys\\\"\\n        self.frequency = frequency\\n        self.verbose = verbose\\n        self.total_batch = 0\\n        self.random_vectors = random_vectors or tf.random.normal(\\n            shape=[n_examples, latent_dim]\\n        )\\n        self.data = list(validation_data.unbatch().take(self.n_examples))\\n\\n        self.recon_raw = self.log_dir / \\\"raw\\\" / \\\"reconstructed\\\"\\n        self.recon_png = self.log_dir / \\\"png\\\" / \\\"reconstructed\\\"\\n        self.gen_raw = self.log_dir / \\\"raw\\\" / \\\"generated\\\"\\n        self.gen_png = self.log_dir / \\\"png\\\" / \\\"generated\\\"\\n\\n    def on_train_begin(self, logs=None):\\n        self.recon_raw.mkdir(exist_ok=True, parents=True)\\n        self.recon_png.mkdir(exist_ok=True, parents=True)\\n        self.gen_raw.mkdir(exist_ok=True, parents=True)\\n        self.gen_png.mkdir(exist_ok=True, parents=True)\\n\\n    def _visualize_reconstruction(self, batch=None, epoch=None):\\n        assert (batch is not None) or (epoch is not None)\\n        for i, sample in enumerate(self.data):\\n            sample = sample[None, :]\\n            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\\n            plt.sca(axes[0])\\n            beatbrain.display.show_spec(\\n                utils.denormalize_spectrogram(sample[0, ..., 0].numpy()),\\n                title=\\\"Original\\\",\\n                cmap=self.cmap,\\n            )\\n            plt.sca(axes[1])\\n            reconstructed = self.model(sample)\\n            beatbrain.display.show_spec(\\n                utils.denormalize_spectrogram(reconstructed[0, ..., 0].numpy()),\\n                title=\\\"Reconstructed\\\",\\n                cmap=self.cmap,\\n            )\\n            fig.tight_layout()\\n            title = f\\\"recon_{i + 1}@{'epoch' if epoch else 'batch'}_{epoch or batch}\\\"\\n            fig.suptitle(title)\\n            plt.savefig(self.recon_png / f\\\"{title}.png\\\")\\n            utils.save_image(\\n                reconstructed[0, ..., 0], self.recon_raw / f\\\"{title}.exr\\\",\\n            )\\n            plt.close()\\n\\n    def _visualize_generation(self, batch=None, epoch=None):\\n        assert (batch is not None) or (epoch is not None)\\n        decoder = self.model.get_layer(\\\"decoder\\\")\\n        generated = decoder(self.random_vectors)\\n        for i, gen in enumerate(generated):\\n            gen = gen[None, :]\\n            fig = plt.figure()\\n            title = f\\\"gen_{i + 1}@{'epoch' if epoch else 'batch'}_{epoch or batch}\\\"\\n            beatbrain.display.show_spec(\\n                utils.denormalize_spectrogram(gen[0, ..., 0].numpy()),\\n                title=title,\\n                cmap=self.cmap,\\n            )\\n            fig.tight_layout()\\n            fig.savefig(self.gen_png / f\\\"{title}.png\\\")\\n            utils.save_image(gen[0, ..., 0], self.gen_raw / f\\\"{title}.exr\\\")\\n            plt.close()\\n\\n    def on_epoch_begin(self, epoch, logs=None):\\n        if self.frequency == \\\"epoch\\\":\\n            self._visualize_reconstruction(epoch=epoch)\\n            self._visualize_generation(epoch=epoch)\\n\\n    def on_train_batch_begin(self, batch, logs=None):\\n        if isinstance(self.frequency, int) and (self.total_batch % self.frequency == 0):\\n            self._visualize_reconstruction(batch=self.total_batch)\\n            self._visualize_generation(batch=self.total_batch)\\n\\n    def on_train_batch_end(self, batch, logs=None):\\n        self.total_batch += 1\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class VisualizeCallback(Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_dir,\n",
    "        latent_dim,\n",
    "        validation_data,\n",
    "        n_examples=4,\n",
    "        random_vectors=None,\n",
    "        heatmap=True,\n",
    "        frequency=\"epoch\",\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_examples = n_examples\n",
    "        self.cmap = None if heatmap else \"Greys\"\n",
    "        self.frequency = frequency\n",
    "        self.verbose = verbose\n",
    "        self.total_batch = 0\n",
    "        self.random_vectors = random_vectors or tf.random.normal(\n",
    "            shape=[n_examples, latent_dim]\n",
    "        )\n",
    "        self.data = list(validation_data.unbatch().take(self.n_examples))\n",
    "\n",
    "        self.recon_raw = self.log_dir / \"raw\" / \"reconstructed\"\n",
    "        self.recon_png = self.log_dir / \"png\" / \"reconstructed\"\n",
    "        self.gen_raw = self.log_dir / \"raw\" / \"generated\"\n",
    "        self.gen_png = self.log_dir / \"png\" / \"generated\"\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.recon_raw.mkdir(exist_ok=True, parents=True)\n",
    "        self.recon_png.mkdir(exist_ok=True, parents=True)\n",
    "        self.gen_raw.mkdir(exist_ok=True, parents=True)\n",
    "        self.gen_png.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def _visualize_reconstruction(self, batch=None, epoch=None):\n",
    "        assert (batch is not None) or (epoch is not None)\n",
    "        for i, sample in enumerate(self.data):\n",
    "            sample = sample[None, :]\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "            plt.sca(axes[0])\n",
    "            beatbrain.display.show_spec(\n",
    "                utils.denormalize_spectrogram(sample[0, ..., 0].numpy()),\n",
    "                title=\"Original\",\n",
    "                cmap=self.cmap,\n",
    "            )\n",
    "            plt.sca(axes[1])\n",
    "            reconstructed = self.model(sample)\n",
    "            beatbrain.display.show_spec(\n",
    "                utils.denormalize_spectrogram(reconstructed[0, ..., 0].numpy()),\n",
    "                title=\"Reconstructed\",\n",
    "                cmap=self.cmap,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            title = f\"recon_{i + 1}@{'epoch' if epoch else 'batch'}_{epoch or batch}\"\n",
    "            fig.suptitle(title)\n",
    "            plt.savefig(self.recon_png / f\"{title}.png\")\n",
    "            utils.save_image(\n",
    "                reconstructed[0, ..., 0], self.recon_raw / f\"{title}.exr\",\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "    def _visualize_generation(self, batch=None, epoch=None):\n",
    "        assert (batch is not None) or (epoch is not None)\n",
    "        decoder = self.model.get_layer(\"decoder\")\n",
    "        generated = decoder(self.random_vectors)\n",
    "        for i, gen in enumerate(generated):\n",
    "            gen = gen[None, :]\n",
    "            fig = plt.figure()\n",
    "            title = f\"gen_{i + 1}@{'epoch' if epoch else 'batch'}_{epoch or batch}\"\n",
    "            beatbrain.display.show_spec(\n",
    "                utils.denormalize_spectrogram(gen[0, ..., 0].numpy()),\n",
    "                title=title,\n",
    "                cmap=self.cmap,\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            fig.savefig(self.gen_png / f\"{title}.png\")\n",
    "            utils.save_image(gen[0, ..., 0], self.gen_raw / f\"{title}.exr\")\n",
    "            plt.close()\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        if self.frequency == \"epoch\":\n",
    "            self._visualize_reconstruction(epoch=epoch)\n",
    "            self._visualize_generation(epoch=epoch)\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        if isinstance(self.frequency, int) and (self.total_batch % self.frequency == 0):\n",
    "            self._visualize_reconstruction(batch=self.total_batch)\n",
    "            self._visualize_generation(batch=self.total_batch)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.total_batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_formatted_code = \"tensorboard = TensorBoard(log_dir=LOG_DIR, update_freq=LOG_FREQUENCY, profile_batch=0,)\\nreduce_lr = ReduceLROnPlateau(patience=2, factor=0.1, min_lr=1e-6, verbose=1,)\\nearly_stop = EarlyStopping(patience=5, verbose=1,)\\nmodel_saver = ModelCheckpoint(\\n    str(MODEL_DIR / MODEL_NAME), save_best_only=True, verbose=1,\\n)\\nvisualizer = VisualizeCallback(\\n    LOG_DIR, LATENT_DIM, val_dataset, frequency=LOG_FREQUENCY\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard = TensorBoard(log_dir=LOG_DIR, update_freq=LOG_FREQUENCY, profile_batch=0,)\n",
    "reduce_lr = ReduceLROnPlateau(patience=2, factor=0.1, min_lr=1e-6, verbose=1,)\n",
    "early_stop = EarlyStopping(patience=5, verbose=1,)\n",
    "model_saver = ModelCheckpoint(\n",
    "    str(MODEL_DIR / MODEL_NAME), save_best_only=True, verbose=1,\n",
    ")\n",
    "visualizer = VisualizeCallback(\n",
    "    LOG_DIR, LATENT_DIM, val_dataset, frequency=LOG_FREQUENCY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Output decoder missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to decoder.\n",
      "Model: \"vae\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(4, 512, 640, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(4, 256), (4, 256), 49683712    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 multiple             1845184     encoder[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (4, 640, 512)        0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (4, 640, 1024)       1573888     lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (4, 640, 512)        1573376     conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (4, 640, 256)        393472      conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (4, 640, 256)        65792       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (4, 640, 256)        65792       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (4, 640, 256)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (4, 640, 256)        65792       conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (4, 640, 256)        196864      conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (4, 640, 256)        327936      conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (4, 640, 256)        65792       max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (4, 640, 1024)       0           conv1d_3[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (4, 320, 1024)       0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (4, 320, 256)        262400      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (4, 320, 256)        262400      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (4, 320, 1024)       0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (4, 320, 256)        262400      max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (4, 320, 256)        196864      conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (4, 320, 256)        327936      conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (4, 320, 256)        262400      max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (4, 320, 1024)       0           conv1d_9[0][0]                   \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (4, 160, 1024)       0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (4, 160, 256)        262400      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (4, 160, 256)        262400      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (4, 160, 1024)       0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (4, 160, 256)        262400      max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (4, 160, 256)        196864      conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (4, 160, 256)        327936      conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (4, 160, 256)        262400      max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (4, 160, 1024)       0           conv1d_15[0][0]                  \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "                                                                 conv1d_19[0][0]                  \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (4, 80, 1024)        0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (4, 81920)           0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (4, 512)             41943552    flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (4, 256)             131328      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (4, 256)             131328      dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (4, 256)             0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_1 (TensorFlowOp [(4, 256)]           0           lambda_1[0][0]                   \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Neg (TensorFlowOpLa [(4, 256)]           0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub (TensorFlowOpLa [(4, 256)]           0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_pow_1 (TensorFlowOp [(4, 256)]           0           tf_op_layer_sub_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Exp (TensorFlowOpLa [(4, 256)]           0           tf_op_layer_Neg[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_pow (TensorFlowOpLa [(4, 256)]           0           tf_op_layer_sub[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_2 (TensorFlowOp [(4, 256)]           0           tf_op_layer_pow_1[0][0]          \n",
      "                                                                 tf_op_layer_Exp[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(4, 256)]           0           tf_op_layer_pow[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_2 (TensorFlowOp [(4, 256)]           0           tf_op_layer_mul_2[0][0]          \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa [(4, 256)]           0           tf_op_layer_mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp [(4, 256)]           0           tf_op_layer_add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_1 (TensorFlowOp [(4, 256)]           0           tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_SquaredDifference ( [(4, 512, 640, 1)]   0           decoder[1][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlowOp [(4, 256)]           0           tf_op_layer_add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_1 (TensorFlowOp [(4, 256)]           0           tf_op_layer_add_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean (TensorFlowOpL [(4, 512, 640)]      0           tf_op_layer_SquaredDifference[0][\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(4,)]               0           tf_op_layer_mul_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(4,)]               0           tf_op_layer_mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(4,)]               0           tf_op_layer_Mean[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_sub_2 (TensorFlowOp [(4,)]               0           tf_op_layer_Sum_2[0][0]          \n",
      "                                                                 tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_4 (TensorFlowOp [(4,)]               0           tf_op_layer_Sum[0][0]            \n",
      "                                                                 tf_op_layer_sub_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mean_1 (TensorFlowO [()]                 0           tf_op_layer_add_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "add_loss (AddLoss)              ()                   0           tf_op_layer_Mean_1[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 51,528,896\n",
      "Trainable params: 51,528,896\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_formatted_code = \"model, encoder, decoder = build_cvae(\\n    LATENT_DIM,\\n    IMAGE_DIMS,\\n    num_conv=NUM_CONV,\\n    batch_size=BATCH_SIZE,\\n    learning_rate=LEARNING_RATE,\\n)\\nmodel.summary()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, encoder, decoder = build_cvae(\n",
    "    LATENT_DIM,\n",
    "    IMAGE_DIMS,\n",
    "    num_conv=NUM_CONV,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layers with arguments in `__init__` must override `get_config`.\n",
      "Epoch 1/50\n",
      " 425/9617 [>.............................] - ETA: 32:58 - loss: 23105.4509"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model.fit_generator(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tensorboard, model_saver, reduce_lr, visualizer,],\n",
    "    validation_data=val_dataset,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
