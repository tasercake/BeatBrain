{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# scientific\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import beatbrain\n",
    "from beatbrain import utils\n",
    "\n",
    "# visualization\n",
    "from IPython import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "sns.set_style(\"white\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "DATA_ROOT = Path(\"../data/fma/image\")\n",
    "TEST_SPLIT = 0.2\n",
    "SUBSET = 5\n",
    "PARALLEL=False\n",
    "\n",
    "# Hyperparameters\n",
    "IMAGE_DIMS = [512, 640, 1]\n",
    "BATCH_SIZE = 1\n",
    "LATENT_DIM = 256\n",
    "EPOCHS = 2\n",
    "NUM_CONV = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Output\n",
    "EXAMPLES_TO_GENERATE = 16\n",
    "INTERPOLATION_POINTS = 9\n",
    "OUTPUT_DIR = Path(\"../data/output/images\")\n",
    "MODEL_PATH = Path(\"model.h5\")\n",
    "TENSORBOARD_DIR = Path('tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking input type(s) in \u001b[33m'/home/cds_data/fma/image'\u001b[39m...\n",
      "Determined input type to be \u001b[36m'IMAGE'\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = utils.load_dataset(\n",
    "    DATA_ROOT,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    test_split=TEST_SPLIT,\n",
    "    parallel=PARALLEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "(1, 512, 640, 1)\n",
      "CPU times: user 1min 47s, sys: 2.18 s, total: 1min 49s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for x in train_dataset.take(SUBSET):\n",
    "    print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cvae(latent_dim, input_shape, num_conv=2, batch_size=1, learning_rate=1e-4):\n",
    "    def reparam(args):\n",
    "        z_mean, z_log_var = args\n",
    "        dim = tf.keras.backend.int_shape(z_mean)[1]\n",
    "        eps = tf.keras.backend.random_normal(shape=(batch_size, dim))\n",
    "        return eps * tf.exp(z_log_var * .5) + z_mean\n",
    "    \n",
    "    def reshape(args):\n",
    "        x = args\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        x = tf.transpose(x, [0, 2, 1])\n",
    "        return x\n",
    "    \n",
    "    def conv1d_transpose(args):\n",
    "        d, conv1d_param = args\n",
    "        filter_size, kernel_size, strides = conv1d_param\n",
    "        decoder_output = tf.nn.conv1d_transpose(d,\n",
    "            output_shape=np.ceil(d.shape[1]/float(strides) ),\n",
    "            filters=filter_size,\n",
    "            strides=strides,\n",
    "            padding='SAME',\n",
    "        )(d)\n",
    "        return decoder_output\n",
    "    \n",
    "    encoder_input = tf.keras.Input(shape=input_shape, batch_size=batch_size)\n",
    "    e = tf.keras.layers.Lambda(reshape)(encoder_input) # (batch_size, time, freq)\n",
    "    e = tf.keras.layers.Conv1D(filters=3, kernel_size=3, strides=2, padding='SAME', activation='relu')(e) # (batch_size, time, filter_size)\n",
    "\n",
    "    for i in range(num_conv):\n",
    "        e = tf.keras.layers.Conv1D(filters=6, kernel_size=3, strides=2, padding='SAME', activation='relu')(e) # (batch_size, time, filter_size)\n",
    "    \n",
    "    decoder_input_shape = tf.keras.backend.int_shape(e)\n",
    "    e = tf.keras.layers.Flatten()(e)\n",
    "    e = tf.keras.layers.Dense(latent_dim*2)(e) # (batch_size, latent_dim*2)\n",
    "    z_mean = tf.keras.layers.Dense(latent_dim)(e)\n",
    "    z_log_var = tf.keras.layers.Dense(latent_dim)(e)\n",
    "    z = tf.keras.layers.Lambda(reparam, output_shape=(latent_dim, ))([z_mean, z_log_var]) # (batch_size, latent_dim)\n",
    "    encoder = tf.keras.Model(encoder_input, [z_mean, z_log_var, z], name=\"Encoder\")\n",
    "\n",
    "#     decoder_input_shape = (batch_size, \n",
    "#         input_shape[0] // (2 ** (num_conv + 1)),\n",
    "#         input_shape[1] // (2 ** (num_conv + 1)),\n",
    "#         64\n",
    "#     )\n",
    "    # (decoder_input_shape): (32, 40, 64)\n",
    "\n",
    "    decoder_input = tf.keras.Input(shape=(latent_dim, ))\n",
    "    d = tf.keras.layers.Dense(\n",
    "        decoder_input_shape[1] * decoder_input_shape[2],\n",
    "        activation=tf.nn.relu,\n",
    "    )(decoder_input)\n",
    "\n",
    "    \n",
    "    d = tf.keras.layers.Reshape(\n",
    "        target_shape=(\n",
    "            decoder_input_shape[1],\n",
    "            decoder_input_shape[2],\n",
    "            )\n",
    "    )(d)\n",
    "    # d.shape: (None, 31, 39, 64)\n",
    "    for i in range(num_conv):\n",
    "#         d = tf.keras.layers.Conv1DTranspose(\n",
    "#             filters=decoder_input_shape[2],\n",
    "#             kernel_size=3,\n",
    "#             strides=2,\n",
    "#             padding='SAME',\n",
    "#             activation='relu',\n",
    "#         )(d)\n",
    "        d = tf.keras.layers.Lambda(conv1d_transpose)([d, [decoder_input_shape[2], 3, 2]])\n",
    "    \n",
    "    d = tf.keras.layers.Lambda(conv1d_transpose)([d, [decoder_input_shape[2], 3, 2]])\n",
    "    decoder_output = tf.keras.layers.Lambda(conv1d_transpose)([d, [decoder_input_shape[2], 3, 1]])\n",
    "\n",
    "    decoder = tf.keras.Model(decoder_input, decoder_output, name=\"Decoder\")    \n",
    "    outputs = decoder(encoder(encoder_input)[2])\n",
    "    model = tf.keras.Model(encoder_input, outputs, name=\"VAE\")\n",
    "    \n",
    "    assert (encoder_input.shape == outputs.shape)\n",
    "    reconstruction_loss = tf.losses.mse(encoder_input, outputs)\n",
    "    reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=[1, 2])\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    logpz = log_normal_pdf(z, 0., 0.)\n",
    "    logqz_x = log_normal_pdf(z, z_mean, z_log_var)\n",
    "    kl_loss = logqz_x - logpz\n",
    "    vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
    "\n",
    "    model.add_loss(vae_loss)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "    return model, encoder, decoder\n",
    "\n",
    "\n",
    "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
    "    log2pi = tf.math.log(2. * np.pi)\n",
    "    return tf.reduce_sum(\n",
    "        -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
    "        axis=raxis\n",
    "    )\n",
    "\n",
    "@tf.function\n",
    "def sample(latent_dim, decoder, eps=None):\n",
    "    if eps is None:\n",
    "        eps = tf.random.normal(shape=(100, latent_dim))\n",
    "    return decode(decoder, eps, apply_sigmoid=True)\n",
    "\n",
    "\n",
    "def encode(encoder, x):\n",
    "    inference = encoder(x)\n",
    "    mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\n",
    "    return mean, logvar\n",
    "\n",
    "\n",
    "def reparameterize(mean, logvar):\n",
    "    eps = tf.random.normal(shape=mean.shape)\n",
    "    return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "\n",
    "def decode(decoder, z, apply_sigmoid=False):\n",
    "    logits = decoder(z)\n",
    "    if apply_sigmoid:\n",
    "        probs = tf.sigmoid(logits)\n",
    "        return probs\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(encoder, decoder, x):\n",
    "    mean, logvar = encoder(x)\n",
    "    z = reparameterize(mean, logvar)\n",
    "    x_logit = decoder(z)\n",
    "    cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
    "    logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3]) # reconstruction\n",
    "    logpz = log_normal_pdf(z, 0., 0.) # kl divergence loss 1 term\n",
    "    logqz_x = log_normal_pdf(z, mean, logvar) # kl divergence loss term\n",
    "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
    "\n",
    "@tf.function\n",
    "def compute_apply_gradients(model, encoder, decoder, x, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(encoder, decoder, x)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Scalar tensor has no `len()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-cedcd6bbd4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# it will be easier to see the improvement.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrandom_vector_for_generation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEXAMPLES_TO_GENERATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLATENT_DIM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_cvae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLATENT_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGE_DIMS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_conv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CONV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-e672329eda84>\u001b[0m in \u001b[0;36mbuild_cvae\u001b[0;34m(latent_dim, input_shape, num_conv, batch_size, learning_rate)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#             activation='relu',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m#         )(d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1d_transpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_input_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1d_transpose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdecoder_input_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training)\u001b[0m\n\u001b[1;32m    793\u001b[0m       \u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mvariable_scope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-e672329eda84>\u001b[0m in \u001b[0;36mconv1d_transpose\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         )(d)\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv1d_transpose\u001b[0;34m(input, filters, output_shape, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m     \u001b[0;31m# Reshape the input tensor to [batch, 1, in_width, in_channels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stride\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m     \u001b[0mdilations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_get_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dilations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m_get_sequence\u001b[0;34m(value, n, channel_index, name)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mcurrent_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcurrent_n\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krishna/anaconda3/envs/ai/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0;34m\"\"\"Returns the length of the first dimension in the Tensor.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scalar tensor has no `len()`\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Scalar tensor has no `len()`"
     ]
    }
   ],
   "source": [
    "# keeping the random vector constant for generation (prediction) so\n",
    "# it will be easier to see the improvement.\n",
    "random_vector_for_generation = tf.random.normal(shape=[EXAMPLES_TO_GENERATE, LATENT_DIM])\n",
    "model, encoder, decoder = build_cvae(LATENT_DIM, IMAGE_DIMS, num_conv=NUM_CONV, batch_size=BATCH_SIZE, learning_rate=LEARNING_RATE)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "    1/34656 [..............................] - ETA: 1083:01:00 - loss: 85054.5312WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.128298). Check your callbacks.\n",
      "23148/34656 [===================>..........] - ETA: 9:46 - loss: 4293.7407"
     ]
    }
   ],
   "source": [
    "# single_sample = utils.load_image('../data/fma/image/000/000002/0.exr')\n",
    "# single_sample = train_dataset.reshape(1, *single_sample.shape, 1)\n",
    "# model.fit(single_sample, epochs=EPOCHS, validation_data=(single_sample, None))\n",
    "\n",
    "# model.fit_generator(train_dataset, epochs=EPOCHS, validation_data=test_dataset)\n",
    "model.fit_generator(train_dataset, epochs=EPOCHS, callbacks=[\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=TENSORBOARD_DIR / str(int(time.time())),\n",
    "        update_freq=100,\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset\n",
    "\n",
    "beatbrain.display.show_spec(utils.denormalize_spectrogram(sample[0, ..., 0]), title=\"Orginal\")\n",
    "plt.show()\n",
    "sns.distplot(train_dataset.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(train_dataset)\n",
    "beatbrain.display.show_spec(utils.denormalize_spectrogram(model(sample)[0, ..., 0].numpy()), title=\"Predicted\")\n",
    "plt.show()\n",
    "sns.distplot(model(train_dataset).numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FFT = 4096\n",
    "HOP_LENGTH = 256\n",
    "SAMPLE_RATE = 32768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_recon = utils.spectrogram_to_audio(sample[0, ..., 0], denormalize=True, n_fft=N_FFT, hop_length=HOP_LENGTH, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(sample_recon, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_recon = utils.spectrogram_to_audio(prediction[0, ..., 0], denormalize=True, n_fft=N_FFT, hop_length=HOP_LENGTH, sr=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(prediction_recon, rate=SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(decoder, epoch, test_input):\n",
    "    num_plots = math.ceil(math.sqrt(len(test_input)))\n",
    "    predictions = sample(LATENT_DIM, decoder, eps=test_input)\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    fig.subplots_adjust(hspace=0, wspace=0)\n",
    "    for i, pred in enumerate(predictions):\n",
    "        plt.subplot(num_plots, num_plots, i + 1)\n",
    "        plt.imshow(pred[:, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "        plt.axis('off')\n",
    "        output_dir = os.path.join(OUTPUT_DIR, 'progress', str(i))\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        image = Image.fromarray(pred[:, :, 0].numpy(), mode='F')\n",
    "#         image.save(os.path.join(output_dir, f\"epoch_{epoch}.tiff\"))\n",
    "        image.save(os.path.join(output_dir, f\"spec.tiff\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import librosa\n",
    "\n",
    "SR = 32768\n",
    "N_FFT = 4096\n",
    "N_MELS = 512\n",
    "HOP_LENGTH = 256\n",
    "DURATION = 5\n",
    "A = \"../data/fma/audio/000002.mp3\"\n",
    "B = \"../data/fma/audio/000005.mp3\"\n",
    "\n",
    "interp_dir = Path(f\"interpolation/{int(time())}\")\n",
    "interp_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "x, _ = librosa.load(A, sr=SR, duration=DURATION)\n",
    "y, _ = librosa.load(B, sr=SR, duration=DURATION)\n",
    "x = librosa.feature.melspectrogram(x, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "y = librosa.feature.melspectrogram(y, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "x, y = x[0, None], y[None, :, None]\n",
    "print(x.shape)\n",
    "x_mean, x_logvar = model.encode(x)\n",
    "y_mean, y_logvar = model.encode(y)\n",
    "\n",
    "# Reconstruction\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "x_recon = model.sample(model.reparameterize(x_mean, x_logvar))\n",
    "plt.imshow(x_recon[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Interpolation\n",
    "fractions = np.linspace(0, 1, num=INTERPOLATION_POINTS)[:, None]\n",
    "means = (x_mean * (1 - fractions)) + (y_mean * fractions)  # Interpolated latent vectors\n",
    "logvars = (x_logvar * (1 - fractions)) + (y_logvar * fractions)  # Interpolated latent vectors\n",
    "points = model.reparameterize(means, logvars)\n",
    "interpolated = model.sample(points)\n",
    "\n",
    "num_plots = math.ceil(math.sqrt(INTERPOLATION_POINTS))\n",
    "fig = plt.figure(figsize=(12, 12))\n",
    "fig.subplots_adjust(hspace=0, wspace=0.03)\n",
    "for i, pred in enumerate(interpolated):\n",
    "    Image.fromarray(pred[:, :, 0].numpy(), mode='F').save(interp_dir.joinpath(f'{i}.tiff'))\n",
    "    plt.subplot(num_plots, num_plots, i + 1)\n",
    "    if i == 0:\n",
    "        Image.fromarray(x[0, :, :, 0].numpy(), mode='F').save(interp_dir.joinpath(f'a.tiff'))\n",
    "        plt.imshow(x[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "    elif i == INTERPOLATION_POINTS - 1:\n",
    "        Image.fromarray(y[0, :, :, 0].numpy(), mode='F').save(interp_dir.joinpath(f'b.tiff'))\n",
    "        plt.imshow(y[0, ..., 0], cmap='gray', vmin=0, vmax=1)\n",
    "    else:\n",
    "        plt.imshow(pred[:, :, 0], cmap='gray', vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.savefig(interp_dir.joinpath('interpolation.png'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unused code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, num_conv=2, input_shape=(*IMAGE_DIMS, WINDOW_SIZE)):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.inference_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=32, kernel_size=3, strides=(2, 2), activation='relu',\n",
    "                ),\n",
    "                *[tf.keras.layers.Conv2D(\n",
    "                    filters=64, kernel_size=3, strides=(2, 2), activation='relu'\n",
    "                ) for i in range(num_conv)],\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(2 * latent_dim)\n",
    "            ], name=\"Encoder\"\n",
    "        )\n",
    "\n",
    "        decoder_input_shape = (\n",
    "            input_shape[0] // (2 ** (num_conv + 1)),\n",
    "            input_shape[1] // (2 ** (num_conv + 1)),\n",
    "            64\n",
    "        )\n",
    "\n",
    "        self.generative_net = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(latent_dim, )),\n",
    "                tf.keras.layers.Dense(\n",
    "                    decoder_input_shape[0] * decoder_input_shape[1] * decoder_input_shape[2],\n",
    "                    activation=tf.nn.relu\n",
    "                ),\n",
    "                tf.keras.layers.Reshape(target_shape=(decoder_input_shape[0], \n",
    "                                                      decoder_input_shape[1], decoder_input_shape[2])),\n",
    "                *[tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=64,\n",
    "                    kernel_size=3,\n",
    "                    strides=(2, 2),\n",
    "                    padding='SAME',\n",
    "                    activation='relu'\n",
    "                ) for i in range(num_conv)],\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32,\n",
    "                    kernel_size=3,\n",
    "                    strides=(2, 2),\n",
    "                    padding='SAME',\n",
    "                    activation='relu'\n",
    "                ),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=1, kernel_size=3, strides=(1, 1), padding='SAME'\n",
    "                )\n",
    "            ], name=\"Decoder\"\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(100, self.latent_dim))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        inference = self.inference_net(x)\n",
    "        mean, logvar = tf.split(inference, num_or_size_splits=2, axis=1)\n",
    "        return mean, logvar\n",
    "\n",
    "    def reparameterize(self, mean, logvar):\n",
    "        eps = tf.random.normal(shape=mean.shape)\n",
    "        return eps * tf.exp(logvar * .5) + mean\n",
    "\n",
    "    def decode(self, z, apply_sigmoid=False):\n",
    "        logits = self.generative_net(z)\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE(LATENT_DIM, num_conv=NUM_CONV,)\n",
    "model.inference_net.summary()\n",
    "model.generative_net.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
